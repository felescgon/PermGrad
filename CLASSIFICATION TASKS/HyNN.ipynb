{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EybOZ6hSjpCF"
   },
   "source": [
    "<h1><font color=\"#113D68\" size=6>PermGrad: Interpretable Hybrid Neural Networks with Synthetic Images for Tabular Data</font></h1>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qwYF5A2njpC8"
   },
   "source": [
    "# <font color=\"#004D7F\" size=6> 1. Libraries</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hy5okt4cpiud"
   },
   "source": [
    "---\n",
    "# <font color=\"#004D7F\" size=5> 1.1. System setup</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "    sudo pip3 install tensorflow==2.17.1 torchmetrics pytorch_lightning TINTOlib==0.0.26 imblearn keras_preprocessing mpi4py bitstring optuna\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i2_nqaiEpiuf"
   },
   "source": [
    "---\n",
    "# <font color=\"#004D7F\" size=5> 1.2. Invoke the libraries</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PeeBbGxlpjFp"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import optuna\n",
    "import gc\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import cv2\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense, Conv2D, Flatten, Dropout, BatchNormalization,\n",
    "    Lambda\n",
    ")\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.utils import plot_model\n",
    "\n",
    "from TINTOlib.tinto import TINTO\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\n",
    "from keras.layers import  BatchNormalization, Input, Concatenate\n",
    "from keras.layers import Concatenate\n",
    "from tensorflow.keras.initializers import HeNormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aDL4LARWjpDT"
   },
   "source": [
    "---\n",
    "# <font color=\"#004D7F\" size=6> 2. Data processing</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pLzynCxOpHBX"
   },
   "source": [
    "---\n",
    "# <font color=\"#004D7F\" size=5> 2.1. TINTO method</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GYHscfDgpiug"
   },
   "outputs": [],
   "source": [
    "dataset = \"Covertype\"\n",
    "pixels=20\n",
    "problem_type = \"supervised\"\n",
    "\n",
    "images_folder = f\"../images/{dataset}\"\n",
    "image_model = TINTO(problem= problem_type,blur=False, pixels=pixels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xtHDA1vxpiug"
   },
   "source": [
    "---\n",
    "# <font color=\"#004D7F\" size=5> 2.2. Read the dataset</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 444
    },
    "executionInfo": {
     "elapsed": 1984,
     "status": "ok",
     "timestamp": 1757257094921,
     "user": {
      "displayName": "Felipe Escalera Gonzalez",
      "userId": "16752102397912297330"
     },
     "user_tz": -120
    },
    "id": "Jp0Zi4qhpiul",
    "outputId": "3bf21369-71e7-4371-ac76-dce889d64a4d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if dataset == \"HELOC\":\n",
    "  dataset_path = \"../datasets/HELOC/heloc.csv\"\n",
    "  df=pd.read_csv(dataset_path, delimiter=',')\n",
    "\n",
    "  column_to_move = df.pop('RiskPerformance')\n",
    "  df['RiskPerformance'] = column_to_move\n",
    "\n",
    "  class_col = df.iloc[:,-1]\n",
    "  df = df.iloc[: , :-1]\n",
    "\n",
    "  label_encoder = LabelEncoder()\n",
    "  class_col_encoded = label_encoder.fit_transform(class_col)\n",
    "\n",
    "\n",
    "if dataset == \"Dengue\":\n",
    "  dataset_path = \"../datasets/Dengue/dengue_chikunguya_bin.csv\"\n",
    "  df=pd.read_csv(dataset_path, delimiter=',')\n",
    "\n",
    "  column_to_move = df.pop('CLASSI_FIN')\n",
    "  df['CLASSI_FIN'] = column_to_move\n",
    "\n",
    "  class_col = df.iloc[:,-1]\n",
    "  df = df.iloc[: , :-1]\n",
    "\n",
    "  label_encoder = LabelEncoder()\n",
    "  class_col_encoded = label_encoder.fit_transform(class_col)\n",
    "\n",
    "\n",
    "if dataset == \"Covertype\":\n",
    "  dataset_path = \"../datasets/Covertype/covtype.csv\"\n",
    "  df=pd.read_csv(dataset_path, delimiter=',')\n",
    "\n",
    "  column_to_move = df.pop('54')\n",
    "  df['54'] = column_to_move\n",
    "\n",
    "  class_col = df.iloc[:,-1]\n",
    "  df = df.iloc[: , :-1]\n",
    "\n",
    "  label_encoder = LabelEncoder()\n",
    "  class_col_encoded = label_encoder.fit_transform(class_col)\n",
    "\n",
    "if dataset == \"Gas\":\n",
    "  dataset_path = \"../datasets/GAS/gas.csv\"\n",
    "  df=pd.read_csv(dataset_path, delimiter=',')\n",
    "\n",
    "  column_to_move = df.pop('Class')\n",
    "  df['Class'] = column_to_move\n",
    "\n",
    "  class_col = df.iloc[:,-1]\n",
    "  df = df.iloc[: , :-1]\n",
    "\n",
    "  label_encoder = LabelEncoder()\n",
    "  class_col_encoded = label_encoder.fit_transform(class_col)\n",
    "\n",
    "\n",
    "df['class'] = class_col_encoded\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1757257094927,
     "user": {
      "displayName": "Felipe Escalera Gonzalez",
      "userId": "16752102397912297330"
     },
     "user_tz": -120
    },
    "id": "lQWix9sjvIPZ",
    "outputId": "32f9fd45-dfb2-47a0-9f2d-a32d5e2ef3da"
   },
   "outputs": [],
   "source": [
    "labels = label_encoder.classes_\n",
    "\n",
    "for label, integer_value in zip(labels, range(len(labels))):\n",
    "    print(f\"Label: {label} -> Integer Value: {integer_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1757257094957,
     "user": {
      "displayName": "Felipe Escalera Gonzalez",
      "userId": "16752102397912297330"
     },
     "user_tz": -120
    },
    "id": "MFzdtvrHI3vU",
    "outputId": "87a41d27-942b-4e40-b031-7383490e277a"
   },
   "outputs": [],
   "source": [
    "class_counts = df['class'].value_counts()\n",
    "\n",
    "print(class_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D9CnSiM5pium"
   },
   "source": [
    "---\n",
    "# <font color=\"#004D7F\" size=5> 2.3. Generate images</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k030GjgEpium"
   },
   "outputs": [],
   "source": [
    "force_recreate_images = True\n",
    "\n",
    "if not os.path.exists(images_folder) or force_recreate_images:\n",
    "    image_model.generateImages(df, images_folder)\n",
    "else:\n",
    "    print(\"The images are already generated\")\n",
    "\n",
    "img_paths = os.path.join(images_folder,problem_type+\".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "klS9PZsUjpDV"
   },
   "source": [
    "---\n",
    "# <font color=\"#004D7F\" size=5> 2.4. Read images</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ql1JjflBtff"
   },
   "outputs": [],
   "source": [
    "imgs = pd.read_csv(img_paths)\n",
    "imgs[\"images\"]= images_folder + \"/\" + imgs[\"images\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ell54BkBlwUh"
   },
   "source": [
    "---\n",
    "# <font color=\"#004D7F\" size=5> 2.5. Mix images and tidy data</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 161
    },
    "executionInfo": {
     "elapsed": 135,
     "status": "ok",
     "timestamp": 1757257121912,
     "user": {
      "displayName": "Felipe Escalera Gonzalez",
      "userId": "16752102397912297330"
     },
     "user_tz": -120
    },
    "id": "MfQ-WgbDlzVh",
    "outputId": "fc5d907e-3ab6-4179-9b4c-eb9ce55f0a90"
   },
   "outputs": [],
   "source": [
    "columns_to_normalize = df.drop(columns='class').columns\n",
    "\n",
    "df_normalized = (df[columns_to_normalize] - df[columns_to_normalize].min()) / (df[columns_to_normalize].max() - df[columns_to_normalize].min())\n",
    "\n",
    "df = pd.concat([df_normalized, df['class']], axis=1)\n",
    "\n",
    "df_normalized.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 57,
     "status": "ok",
     "timestamp": 1757257121979,
     "user": {
      "displayName": "Felipe Escalera Gonzalez",
      "userId": "16752102397912297330"
     },
     "user_tz": -120
    },
    "id": "gGGoc-CT9vpn",
    "outputId": "87ddd937-2bb4-4510-b4d7-0e2a1ac8b5c8"
   },
   "outputs": [],
   "source": [
    "combined_dataset = pd.concat([imgs,df.iloc[:, :-1]],axis=1)\n",
    "\n",
    "df_x = combined_dataset.drop(\"class\",axis=1)\n",
    "df_y = combined_dataset[\"class\"]\n",
    "\n",
    "print(df_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DoEeYhAtpiun"
   },
   "source": [
    "---\n",
    "# <font color=\"#004D7F\" size=6> 3. Pre-modelling phase</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nb4Dd37rjpDm"
   },
   "source": [
    "---\n",
    "# <font color=\"#004D7F\" size=5> 3.1. Data curation</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1446,
     "status": "ok",
     "timestamp": 1757257123423,
     "user": {
      "displayName": "Felipe Escalera Gonzalez",
      "userId": "16752102397912297330"
     },
     "user_tz": -120
    },
    "id": "BDyHty4-pjF5",
    "outputId": "3e34ea2f-74f7-4b19-86dd-f5108526a44e"
   },
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(df_x, df_y, test_size = 0.40, random_state = 42,stratify=df_y)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size = 0.50, random_state = 42,stratify=y_val)\n",
    "\n",
    "X_train_num = X_train.drop(\"images\",axis=1)\n",
    "X_val_num = X_val.drop(\"images\",axis=1)\n",
    "X_test_num = X_test.drop(\"images\",axis=1)\n",
    "\n",
    "X_train_img = np.array([cv2.resize(cv2.imread(img),(pixels,pixels)) for img in X_train[\"images\"]])\n",
    "X_val_img = np.array([cv2.resize(cv2.imread(img),(pixels,pixels)) for img in X_val[\"images\"]])\n",
    "X_test_img = np.array([cv2.resize(cv2.imread(img),(pixels,pixels)) for img in X_test[\"images\"]])\n",
    "\n",
    "n_class = df['class'].value_counts().count()\n",
    "attributes = len(X_train_num.columns)\n",
    "\n",
    "print(\"Image shape\",X_train_img[0].shape)\n",
    "print(\"Attributes\",attributes)\n",
    "print(\"Classes\",n_class)\n",
    "size=X_train_img[0].shape[0]\n",
    "print(\"Image size (pixels):\", pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HD-PEnwn-C17"
   },
   "outputs": [],
   "source": [
    "y_train_oh =  to_categorical(y_train,n_class)\n",
    "y_val_oh = to_categorical(y_val,n_class)\n",
    "y_test_oh = to_categorical(y_test,n_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D5QkishzCEL7"
   },
   "outputs": [],
   "source": [
    "df_train = pd.concat([X_train, y_train], axis = 1)\n",
    "df_test = pd.concat([X_test, y_test], axis = 1)\n",
    "df_val = pd.concat([X_val, y_val], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1757257123471,
     "user": {
      "displayName": "Felipe Escalera Gonzalez",
      "userId": "16752102397912297330"
     },
     "user_tz": -120
    },
    "id": "ebOBC5iypjGA",
    "outputId": "19838ab1-e340-46de-9191-9c1c2280bdb8"
   },
   "outputs": [],
   "source": [
    "print(df_train.shape)\n",
    "print(df_val.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rE0VunQYjpEY"
   },
   "source": [
    "---\n",
    "# <font color=\"#004D7F\" size=6> 4. Modelling with CNN</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qn2U90FwjpEe"
   },
   "source": [
    "---\n",
    "# <font color=\"#004D7F\" size=5> 4.1. CNN</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = f'../models/{dataset}/model_{dataset}_hybrid.keras'\n",
    "\n",
    "study_db_path = f'../models/{dataset}/study_{dataset}_hybrid.db'\n",
    "storage_url = f\"sqlite:///{study_db_path}\"\n",
    "study_name = f\"{dataset}_cnn_study\"\n",
    "\n",
    "base_checkpoint_dir = f'../datasets/{dataset}/optuna_hybrid_checkpoints'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tZl0AETUw1r8"
   },
   "outputs": [],
   "source": [
    "def create_multimodal_classifier(trial, shape, pixels, n_class):\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.4)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=True)\n",
    "    n_dense_layers = trial.suggest_int(\"n_dense_layers\", 1, 4)\n",
    "    n_conv_layers = trial.suggest_int(\"n_conv_layers\", 1, 4)\n",
    "    base_filters = trial.suggest_int(\"base_filters\", 8, 256)\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"adam\", \"adamw\"])\n",
    "    activation_fn = trial.suggest_categorical(\"activation\", [\"relu\"])\n",
    "\n",
    "    init_name = trial.suggest_categorical(\"initializer\", [\"he_normal\"])\n",
    "    if init_name == \"he_normal\":\n",
    "        initializer = HeNormal()\n",
    "\n",
    "    ff_inputs = Input(shape=(shape,))\n",
    "    x_tab = ff_inputs\n",
    "    for i in range(n_dense_layers):\n",
    "        units = trial.suggest_int(f\"dense_units_{i}\", 8, 256)\n",
    "        x_tab = Dense(units, activation=activation_fn, kernel_initializer=initializer)(x_tab)\n",
    "        x_tab = BatchNormalization()(x_tab)\n",
    "        x_tab = Dropout(dropout)(x_tab)\n",
    "    ff_model = Model(ff_inputs, x_tab)\n",
    "\n",
    "    cnn_inputs = Input(shape=(pixels, pixels, 3))\n",
    "    x_cnn = cnn_inputs\n",
    "\n",
    "    for i in range(n_conv_layers):\n",
    "        filters = int(base_filters * (2 ** i))\n",
    "        x_cnn = Conv2D(filters, (3, 3), activation=activation_fn, padding='same', kernel_initializer=initializer)(x_cnn)\n",
    "        x_cnn = BatchNormalization()(x_cnn)\n",
    "        x_cnn = MaxPooling2D(2, 2)(x_cnn)\n",
    "        x_cnn = Dropout(dropout)(x_cnn)\n",
    "    x_cnn = Flatten()(x_cnn)\n",
    "    cnn_model = Model(cnn_inputs, x_cnn)\n",
    "\n",
    "    combined = Concatenate()([ff_model.output, cnn_model.output])\n",
    "    x_conc = combined\n",
    "    for i in range(n_dense_layers):\n",
    "        units = trial.suggest_int(f\"combined_dense_units_{i}\", 8, 256)\n",
    "        x_conc = Dense(units, activation=activation_fn, kernel_initializer=initializer)(x_conc)\n",
    "        x_conc = BatchNormalization()(x_conc)\n",
    "        x_conc = Dropout(dropout)(x_conc)\n",
    "\n",
    "    output = Dense(n_class, activation='softmax', kernel_initializer=initializer)(x_conc)\n",
    "\n",
    "    if optimizer_name == \"adam\":\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    else:\n",
    "        wd = trial.suggest_float(\"weight_decay\", 1e-6, 1e-3, log=True)\n",
    "        opt = tf.keras.optimizers.AdamW(learning_rate=lr, weight_decay=wd)\n",
    "\n",
    "    model = Model(inputs=[ff_model.input, cnn_model.input], outputs=output)\n",
    "    model.compile(\n",
    "        optimizer=opt,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=[\n",
    "            tf.keras.metrics.CategoricalAccuracy(name='accuracy'),\n",
    "            tf.keras.metrics.Precision(name='precision'),\n",
    "            tf.keras.metrics.Recall(name='recall'),\n",
    "            tf.keras.metrics.AUC(name='auc'),\n",
    "        ]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def one_cycle_schedule(epoch, lr, total_epochs, max_lr, min_lr=1e-5):\n",
    "    if epoch < total_epochs * 0.25:\n",
    "        return min_lr + (max_lr - min_lr) * (epoch / (total_epochs * 0.25))\n",
    "    else:\n",
    "        progress = (epoch - total_epochs * 0.25) / (total_epochs * 0.75)\n",
    "        return max_lr * 0.5 * (1 + np.cos(np.pi * progress))\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    os.makedirs(base_checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    checkpoint_path = os.path.join(base_checkpoint_dir, f\"trial_{trial.number}_best_model.keras\")\n",
    "\n",
    "    shape = len(X_train_num.columns)\n",
    "    pixels = X_train_img.shape[1]\n",
    "    n_class = len(np.unique(y_train)) if len(np.unique(y_train)) > 2 else 2\n",
    "\n",
    "    model = create_multimodal_classifier(trial, shape, pixels, n_class)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [32])\n",
    "    epochs = 70\n",
    "\n",
    "    max_lr = trial.params.get(\"lr\", 1e-2)\n",
    "    lr_scheduler = tf.keras.callbacks.LearningRateScheduler(\n",
    "        lambda epoch, lr: one_cycle_schedule(epoch, lr, total_epochs=epochs, max_lr=max_lr),\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "        checkpoint_path,\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        save_weights_only=False,\n",
    "        mode='min',\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=6, restore_best_weights=True\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        [X_train_num, X_train_img], y_train_oh,\n",
    "        validation_data=([X_val_num, X_val_img], y_val_oh),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        verbose=1,\n",
    "        callbacks=[early_stopping, checkpoint_cb, lr_scheduler]\n",
    "    )\n",
    "\n",
    "    val_loss = min(history.history['val_loss'])\n",
    "    trial.set_user_attr(\"best_model_path\", checkpoint_path)\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wAwbZKpEpivK",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "force_retrain = False\n",
    "\n",
    "if not os.path.exists(model_path) or force_retrain:\n",
    "\n",
    "    print(f\"Creating or loading study: {study_name} from {study_db_path}\")\n",
    "    study = optuna.create_study(\n",
    "        study_name=study_name,\n",
    "        storage=storage_url,\n",
    "        direction=\"minimize\",\n",
    "        load_if_exists=True\n",
    "    )\n",
    "\n",
    "    n_total_trials = 50\n",
    "    print(f\"Current trials: {len(study.trials)}. Optimizing up to {n_total_trials} total trials.\")\n",
    "    study.optimize(objective, n_trials=(n_total_trials - len(study.trials)))\n",
    "\n",
    "    print(\"\\nOptimization complete. Best trial:\")\n",
    "    best_trial = study.best_trial\n",
    "    print(f\"  Value (val_loss): {best_trial.value}\")\n",
    "    for key, value in best_trial.params.items():\n",
    "        print(f\"    {key}: {value}\")\n",
    "\n",
    "    best_model_path = best_trial.user_attrs[\"best_model_path\"]\n",
    "    print(f\"Loading best model from: {best_model_path}\")\n",
    "    best_model = load_model(best_model_path)\n",
    "\n",
    "    best_model.save(model_path)\n",
    "    model = best_model\n",
    "    print(f\"Best model saved to: {model_path}\")\n",
    "\n",
    "else:\n",
    "    print(f\"Model already exists at {model_path}. Loading it.\")\n",
    "    model = load_model(model_path)\n",
    "\n",
    "print(\"Process finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(\n",
    "    model,\n",
    "    show_shapes=True,\n",
    "    show_layer_names=True,\n",
    "    expand_nested=True,\n",
    "    dpi=96\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9R2jytfijpEp"
   },
   "source": [
    "---\n",
    "# <font color=\"#004D7F\" size=6> 5. Results</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cQq3wYejpivM"
   },
   "source": [
    "---\n",
    "<a id=\"section62\"></a>\n",
    "# <font color=\"#004D7F\" size=5> 5.1. Validation/Test evaluation</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6791,
     "status": "ok",
     "timestamp": 1757257135673,
     "user": {
      "displayName": "Felipe Escalera Gonzalez",
      "userId": "16752102397912297330"
     },
     "user_tz": -120
    },
    "id": "MtDLUW_QpjGO",
    "outputId": "256b3165-5866-4196-8763-e6477552912a"
   },
   "outputs": [],
   "source": [
    "#score_test = model.evaluate([X_val_num, X_val_img], y_val_oh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2864,
     "status": "ok",
     "timestamp": 1757257138538,
     "user": {
      "displayName": "Felipe Escalera Gonzalez",
      "userId": "16752102397912297330"
     },
     "user_tz": -120
    },
    "id": "27tFyGE8pjGO",
    "outputId": "e1a9f4ac-1768-4026-f5e1-9db82e74d50e"
   },
   "outputs": [],
   "source": [
    "#score_test = model.evaluate([X_test_num, X_test_img], y_test_oh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#score_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F8GRJuLKpivT"
   },
   "source": [
    "---\n",
    "# <font color=\"#004D7F\" size=6> 6. PermGrad Framework application</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zqU8W1mjM7D-"
   },
   "source": [
    "---\n",
    "## <font color=\"#004D7F\" size=6> 6.1. Permutation Feature Importance - MLP</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1UKJRIHKv2sN"
   },
   "outputs": [],
   "source": [
    "def plot_stacked_feature_relevance_bar(category_feature_importance, title, output_path, top_n=10):\n",
    "    summed_metrics = {feature: 0 for feature in next(iter(category_feature_importance.values())).keys()}\n",
    "\n",
    "    for group, metrics in category_feature_importance.items():\n",
    "        for key, value in metrics.items():\n",
    "            summed_metrics[key] += value\n",
    "\n",
    "    sorted_dict = dict(sorted(summed_metrics.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "    top_features_dict = dict(list(sorted_dict.items())[:top_n])\n",
    "    print(f\"Top {top_n} Features: {top_features_dict}\")\n",
    "\n",
    "    categories = sorted(category_feature_importance.keys())\n",
    "\n",
    "    all_features = set(f for data in category_feature_importance.values() for f in data.keys())\n",
    "\n",
    "    raw_scores = {cat: {} for cat in categories}\n",
    "    for category, feature_scores in category_feature_importance.items():\n",
    "        for feature, score in feature_scores.items():\n",
    "            value = score.numpy() if hasattr(score, 'numpy') else score\n",
    "            raw_scores[category][feature] = value\n",
    "\n",
    "    feature_total_values = {\n",
    "        feature: sum(raw_scores.get(cat, {}).get(feature, 0) for cat in categories)\n",
    "        for feature in all_features\n",
    "    }\n",
    "\n",
    "    sorted_features = sorted(\n",
    "        feature_total_values.keys(),\n",
    "        key=lambda f: feature_total_values[f],\n",
    "        reverse=True\n",
    "    )\n",
    "\n",
    "    sorted_features = sorted_features[:top_n]\n",
    "\n",
    "    num_features = len(sorted_features)\n",
    "    y_positions = np.arange(num_features)\n",
    "\n",
    "    cmap = plt.get_cmap('tab10' if len(categories) > 5 else 'Accent')\n",
    "    colors = [cmap(i) for i in range(len(categories))]\n",
    "    alpha=1.0\n",
    "    if len(categories) > 5:\n",
    "        alpha = 0.55\n",
    "\n",
    "    left_offset = np.zeros(num_features)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    for i, category in enumerate(categories):\n",
    "        values = [raw_scores.get(category, {}).get(f, 0) for f in sorted_features]\n",
    "\n",
    "        plt.barh(\n",
    "            y_positions,\n",
    "            values,\n",
    "            left=left_offset,\n",
    "            height=0.8,\n",
    "            label=category,\n",
    "            color=colors[i],\n",
    "            edgecolor='white',\n",
    "            linewidth=0.5,\n",
    "            alpha=alpha\n",
    "        )\n",
    "        left_offset += np.array(values)\n",
    "\n",
    "    plt.axvline(x=0, color='grey', linestyle='--', linewidth=0.8)\n",
    "    plt.ylabel(\"Features\", fontsize=18)\n",
    "    plt.xlabel(\"Feature Importance\", fontsize=18)\n",
    "    if title != \"\":\n",
    "        plt.title(f\"{title}\", fontsize=18)\n",
    "        \n",
    "    plt.xticks(fontsize=14)\n",
    "    plt.yticks(y_positions, sorted_features, size=14)\n",
    "\n",
    "    plt.gca().invert_yaxis()\n",
    "\n",
    "    plt.legend(title=\"Category\", bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=12, title_fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path)\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1757264434652,
     "user": {
      "displayName": "Felipe Escalera Gonzalez",
      "userId": "16752102397912297330"
     },
     "user_tz": -120
    },
    "id": "nnGhsCinM_He",
    "outputId": "2eb66257-ac84-46cb-f689-458afe34b81c"
   },
   "outputs": [],
   "source": [
    "if dataset == \"HELOC\":\n",
    "  dataset_path = \"../datasets/HELOC\"\n",
    "  mlp_importances_named_path = f'{dataset_path}/mlp_metrics.npy'\n",
    "if dataset == \"Dengue\":\n",
    "  dataset_path = \"../datasets/DENGUE_CHIKUNGUNYA\"\n",
    "  mlp_importances_named_path = f'{dataset_path}/mlp_metrics.npy'\n",
    "if dataset == \"Covertype\":\n",
    "  dataset_path = \"../datasets/Covertype\"\n",
    "  mlp_importances_named_path = f'{dataset_path}/mlp_metrics.npy'\n",
    "if dataset == \"Gas\":\n",
    "  dataset_path = \"../datasets/Gas\"\n",
    "  mlp_importances_named_path = f'{dataset_path}/mlp_metrics.npy'\n",
    "\n",
    "n_repeats = 5\n",
    "\n",
    "if not os.path.exists(mlp_importances_named_path):\n",
    "  y_true = np.argmax(y_test_oh, axis=1)\n",
    "  class_labels = np.unique(y_true)\n",
    "  n_classes = len(class_labels)\n",
    "\n",
    "  y_probas = model.predict([X_test_num, X_test_img], verbose=0)\n",
    "  y_pred   = np.argmax(y_probas, axis=1)\n",
    "\n",
    "  mlp_importances = {c: {} for c in class_labels}\n",
    "\n",
    "  baselines = {}\n",
    "  for c in class_labels:\n",
    "      idx = np.where(y_true == c)[0]\n",
    "      mlp_losses = tf.keras.losses.categorical_crossentropy(\n",
    "        y_test_oh[idx], y_probas[idx]\n",
    "      )\n",
    "      baselines[c] = mlp_losses\n",
    "\n",
    "  for feature in tqdm(X_test_num.columns, desc=\"Permuting features\"):\n",
    "      deltas = {c: [] for c in class_labels}\n",
    "\n",
    "      for _ in range(n_repeats):\n",
    "          X_perm = X_test_num.copy()\n",
    "          X_perm[feature] = np.random.permutation(X_perm[feature].values)\n",
    "\n",
    "          y_probas_perm = model.predict([X_perm, X_test_img], verbose=0)\n",
    "\n",
    "          for c in class_labels:\n",
    "              idx = np.where(y_true == c)[0]\n",
    "              loss_perm_c = tf.keras.losses.categorical_crossentropy(\n",
    "                  y_test_oh[idx], y_probas_perm[idx]\n",
    "              )\n",
    "              deltas[c].append(np.mean(loss_perm_c) - baselines[c])\n",
    "\n",
    "      for c in class_labels:\n",
    "          mlp_importances[c][feature] = np.mean(deltas[c])\n",
    "\n",
    "  mlp_importances_named = {label_encoder.inverse_transform([c])[0]: val for c, val in mlp_importances.items()}\n",
    "\n",
    "  np.save(mlp_importances_named_path, mlp_importances_named, allow_pickle=True)\n",
    "\n",
    "else:\n",
    "  mlp_importances_named = np.load(mlp_importances_named_path, allow_pickle=True).item()\n",
    "\n",
    "print(f\"\\n=== MLP‐branch ΔAccuracy per class (features as columns, averaged over {n_repeats} repeats) ===\")\n",
    "print(mlp_importances_named)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "mlp_importances_named_path = f'../datasets/Covertype/mlp_metrics.npy'\n",
    "np.load(mlp_importances_named_path, allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 644
    },
    "executionInfo": {
     "elapsed": 788,
     "status": "ok",
     "timestamp": 1757264440179,
     "user": {
      "displayName": "Felipe Escalera Gonzalez",
      "userId": "16752102397912297330"
     },
     "user_tz": -120
    },
    "id": "qLRrMaFWNGRW",
    "outputId": "70a2975a-fdc5-4fb9-d94f-3316a78c729e"
   },
   "outputs": [],
   "source": [
    "all_values = [val for class_dict in mlp_importances_named.values() for val in class_dict.values()]\n",
    "min_val, max_val = min(all_values), max(all_values)\n",
    "\n",
    "mlp_importances_normalized = {\n",
    "    feature: {c: (val - min_val) / (max_val - min_val) for c, val in class_dict.items()}\n",
    "    for feature, class_dict in mlp_importances_named.items()\n",
    "}\n",
    "\n",
    "plot_stacked_feature_relevance_bar(mlp_importances_normalized, title=\"\", output_path=f'{dataset_path}/mlp_metrics.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FdLz_EfWNINm"
   },
   "source": [
    "---\n",
    "## <font color=\"#004D7F\" size=6> 6.2. Grad-CAM Heatmap Feature Importance - CNN</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 959
    },
    "executionInfo": {
     "elapsed": 824,
     "status": "ok",
     "timestamp": 1757264436101,
     "user": {
      "displayName": "Felipe Escalera Gonzalez",
      "userId": "16752102397912297330"
     },
     "user_tz": -120
    },
    "id": "eVXtQhOnNJDW",
    "outputId": "8d6444ca-c444-48e6-e297-0fe32c5783c5"
   },
   "outputs": [],
   "source": [
    "def cuadrado(coord):\n",
    "    m = np.mean(coord, axis=0).reshape((1, 2))\n",
    "    coord_nuevo = coord - m\n",
    "    dista = (coord_nuevo[:, 0]**2 + coord_nuevo[:, 1]**2)**0.5\n",
    "    maxi = math.ceil(max(dista))\n",
    "    vertices = np.array([[-maxi, maxi], [-maxi, -maxi], [maxi, -maxi], [maxi, maxi]])\n",
    "    coord_nuevo = coord_nuevo - vertices[0]\n",
    "    vertices = vertices - vertices[0]\n",
    "    return coord_nuevo, vertices\n",
    "\n",
    "def m_imagen(coord, vertices, pixeles=20):\n",
    "    size = (pixeles, pixeles)\n",
    "    matriz = np.zeros(size)\n",
    "    coord_m = (coord / vertices[2, 0]) * (pixeles - 1)\n",
    "    coord_m = np.round(abs(coord_m))\n",
    "    for i, j in zip(coord_m[:, 1], coord_m[:, 0]):\n",
    "        matriz[int(i), int(j)] = 1\n",
    "    if np.count_nonzero(matriz != 0) != coord.shape[0]:\n",
    "        return coord_m, matriz, True\n",
    "    else:\n",
    "        return coord_m, matriz, False\n",
    "\n",
    "\n",
    "class DataImg:\n",
    "    def __init__(self, algoritmo='PCA', pixeles=20, seed=20, veces=4, amp=np.pi, distancia=0.1, pasos=4, opcion='maximo'):\n",
    "        self.algoritmo = algoritmo\n",
    "        self.p = pixeles\n",
    "        self.seed = seed\n",
    "        self.veces = veces\n",
    "\n",
    "        self.amp = amp\n",
    "        self.distancia = distancia\n",
    "        self.pasos = pasos\n",
    "        self.opcion = opcion\n",
    "\n",
    "        self.error_pos = False\n",
    "\n",
    "    def ObtenerCoord(self, X):\n",
    "        self.min_max_scaler = MinMaxScaler()\n",
    "        X = self.min_max_scaler.fit_transform(X)\n",
    "        labels = np.arange(X.shape[1])\n",
    "        X_trans = X.T\n",
    "\n",
    "\n",
    "        if(self.algoritmo=='PCA'):\n",
    "            X_embedded = PCA(n_components=2,random_state=self.seed).fit(X_trans).transform(X_trans)\n",
    "        elif(self.algoritmo=='t-SNE'):\n",
    "            for _ in range(self.veces):\n",
    "                X_trans = np.append(X_trans,X_trans,axis=0)\n",
    "                labels = np.append(labels,labels,axis=0)\n",
    "            X_embedded = TSNE(n_components=2,random_state=self.seed,perplexity=50).fit_transform(X_trans)\n",
    "        else:\n",
    "            X_embedded = np.random.rand(X.shape[1],2)\n",
    "\n",
    "        datos_coordenadas = {'x':X_embedded[:,0], 'y':X_embedded[:,1], 'Sector':labels}\n",
    "        dc = pd.DataFrame(data=datos_coordenadas)\n",
    "        self.coord_obtenidas = dc.groupby('Sector').mean().values\n",
    "\n",
    "        del X_trans\n",
    "        gc.collect()\n",
    "\n",
    "    def Delimitacion(self):\n",
    "        self.coordenadas_iniciales, self.vertices = cuadrado(self.coord_obtenidas)\n",
    "\n",
    "    def ObtenerMatrizPosiciones(self, columns_names):\n",
    "        self.pos_pixel_caract, self.m, self.error_pos = m_imagen(self.coordenadas_iniciales,self.vertices,pixeles=self.p)\n",
    "        self.columns_coords = dict()\n",
    "\n",
    "        for coord, column_name in zip(self.pos_pixel_caract, columns_names):\n",
    "          self.columns_coords[column_name] = coord\n",
    "\n",
    "        print(self.columns_coords)\n",
    "\n",
    "    def Entrenamiento(self, X, columns_names):\n",
    "        self.columns_names = columns_names\n",
    "        self.ObtenerCoord(X)\n",
    "        self.Delimitacion()\n",
    "        self.ObtenerMatrizPosiciones(columns_names)\n",
    "\n",
    "    def CrearImagenSinteticaConColores(self, pixels=20, column_names=None):\n",
    "        matriz = np.ones((pixels, pixels, 3))\n",
    "\n",
    "        colores = mpl.colormaps['tab20'](np.linspace(0, 1, len(column_names)))\n",
    "\n",
    "        if hasattr(self, 'pos_pixel_caract') and self.pos_pixel_caract is not None and column_names is not None:\n",
    "            for i, pos in enumerate(self.pos_pixel_caract):\n",
    "                if i < len(column_names):\n",
    "                    x, y = int(pos[0]), int(pos[1])\n",
    "                    if x < pixels and y < pixels:\n",
    "                        matriz[x, y] = colores[i][:3]\n",
    "\n",
    "        patches = [mpatches.Patch(color=colores[i][:3], label=column_names[i]) for i in range(len(column_names))]\n",
    "\n",
    "        plt.imshow(matriz)\n",
    "        plt.legend(handles=patches, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "        plt.show()\n",
    "\n",
    "df_x=pd.read_csv(dataset_path, delimiter=',')\n",
    "\n",
    "if dataset == \"HELOC\":\n",
    "  column_to_move = df_x.pop('RiskPerformance')\n",
    "\n",
    "if dataset == \"Dengue\":\n",
    "  column_to_move = df_x.pop('CLASSI_FIN')\n",
    "\n",
    "if dataset == \"Covertype\":\n",
    "  column_to_move = df_x.pop('54')\n",
    "\n",
    "if dataset == \"Gas\":\n",
    "  column_to_move = df_x.pop('Class')\n",
    "\n",
    "modeloIMG = DataImg(algoritmo=\"PCA\", pixeles=pixels, amp=np.pi, distancia=2, pasos=4, opcion='mean', seed=42, veces=4)\n",
    "\n",
    "df_x_sec = df_x\n",
    "modeloIMG.Entrenamiento(df_x_sec, df_x_sec.columns.values)\n",
    "modeloIMG.CrearImagenSinteticaConColores(pixels=pixels, column_names=df_x_sec.columns.values)\n",
    "modeloIMG.columns_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 963
    },
    "executionInfo": {
     "elapsed": 393,
     "status": "ok",
     "timestamp": 1757264436495,
     "user": {
      "displayName": "Felipe Escalera Gonzalez",
      "userId": "16752102397912297330"
     },
     "user_tz": -120
    },
    "id": "bCZdlYIPwKjq",
    "outputId": "8614673f-d168-4caf-9a33-59ca7c0ea58f"
   },
   "outputs": [],
   "source": [
    "def display_image_with_labels(ax, image, column_coords,\n",
    "                              max_labels_per_pixel=1,\n",
    "                              stack_spacing=0.5,\n",
    "                              max_label_length=11, fontsize=14):\n",
    "    pixels = image.shape[0]\n",
    "    ax.imshow(image)\n",
    "\n",
    "    pixel_map = {}\n",
    "    short_to_long = {}\n",
    "\n",
    "    i = 1\n",
    "    for label, coords in column_coords.items():\n",
    "        row, col = int(coords[0]), int(coords[1])\n",
    "        if 0 <= row < pixels and 0 <= col < pixels:\n",
    "            vname = f\"V{i}\"\n",
    "            pixel_map.setdefault((row, col), []).append((vname, label))\n",
    "            short_to_long[vname] = label\n",
    "            i += 1\n",
    "\n",
    "    def _clamp(v, lo, hi):\n",
    "        return max(lo, min(hi, v))\n",
    "\n",
    "    cluster_id = 0\n",
    "    cluster_map = {}\n",
    "\n",
    "    for (row, col), labels in pixel_map.items():\n",
    "        k = len(labels)\n",
    "\n",
    "        if k <= max_labels_per_pixel:\n",
    "            start_y = row - (k - 1) * stack_spacing / 2.0\n",
    "            for i, (short_name, long_name) in enumerate(labels):\n",
    "                label_text = short_name\n",
    "                if len(label_text) > max_label_length:\n",
    "                    label_text = label_text[:max_label_length-1] + \"…\"\n",
    "\n",
    "                plot_y = start_y + i * stack_spacing\n",
    "                clamped_x = _clamp(col, -0.5, pixels - 0.5)\n",
    "                clamped_y = _clamp(plot_y, -0.5, pixels - 0.5)\n",
    "\n",
    "                ax.text(clamped_x, clamped_y - 1, label_text,\n",
    "                        ha='center', va='center', fontsize=fontsize, zorder=10,\n",
    "                        bbox=dict(facecolor='white', alpha=0.45, edgecolor='none', pad=0.6),\n",
    "                        clip_on=True)\n",
    "\n",
    "        else:\n",
    "            cluster_name = f\"C{cluster_id}\"\n",
    "            cluster_contents = [long for _, long in labels]\n",
    "            cluster_map[cluster_name] = cluster_contents\n",
    "\n",
    "            ax.text(col, row - 1, cluster_name,\n",
    "                    ha='center', va='center', fontsize=fontsize, fontweight='bold', zorder=10,\n",
    "                    bbox=dict(facecolor='white', alpha=0.45, edgecolor='none', pad=0.8),\n",
    "                    clip_on=True)\n",
    "\n",
    "            cluster_id += 1\n",
    "\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "    print(\"\\n--- Label Translation Dictionary ---\")\n",
    "    for v, full in short_to_long.items():\n",
    "        print(f\"{v} -> {full}\")\n",
    "    for c, full_list in cluster_map.items():\n",
    "        print(f\"{c} -> {', '.join(full_list)}\")\n",
    "    print(\"------------------------------------\")\n",
    "\n",
    "    return short_to_long, cluster_map\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(4, 4))\n",
    "sample_image = X_test_img[7]\n",
    "display_image_with_labels(ax, sample_image, modeloIMG.columns_coords, max_labels_per_pixel=1)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{dataset_path}/tinto_sample.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wk9fx_4fNT2D"
   },
   "outputs": [],
   "source": [
    "def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None, consider_multiclass=False):\n",
    "    grad_model = keras.models.Model(\n",
    "        model.inputs, [model.get_layer(last_conv_layer_name).output, model.output]\n",
    "    )\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        last_conv_layer_output, preds = grad_model(img_array)\n",
    "        if not consider_multiclass:\n",
    "            pred_index = tf.argmax(preds[0])\n",
    "        class_channel = preds[:, pred_index]\n",
    "\n",
    "    grads = tape.gradient(class_channel, last_conv_layer_output)\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "\n",
    "    last_conv_layer_output = last_conv_layer_output[0]\n",
    "    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n",
    "    heatmap = tf.squeeze(heatmap)\n",
    "    return heatmap\n",
    "\n",
    "def get_normalized_heatmap_metrics(X_num, X_img, model, layer_name, classes, column_coords, model_type=\"HYBRID\"):\n",
    "    metrics_dict = {class_label: {column: [] for column in X_num.columns.tolist()} for class_label in classes}\n",
    "\n",
    "    pixels = X_img.shape[1]\n",
    "\n",
    "    for i in tqdm(range(len(X_num)), desc=\"Calculando Métricas de Heatmap\"):\n",
    "        for pred_index in range(len(classes)):\n",
    "            if model_type == \"HYBRID\":\n",
    "                num_input = np.expand_dims(X_num.iloc[i], axis=0)\n",
    "                img_input = np.expand_dims(X_img[i], axis=0)\n",
    "                heatmap = make_gradcam_heatmap([num_input, img_input], model, layer_name, pred_index=pred_index)\n",
    "            elif model_type == \"CNN\":\n",
    "                img_input = np.expand_dims(X_img[i], axis=0)\n",
    "                heatmap = make_gradcam_heatmap(img_input, model, layer_name, pred_index=pred_index)\n",
    "\n",
    "            heatmap = tf.maximum(heatmap, 0)\n",
    "            max_val = tf.math.reduce_max(heatmap)\n",
    "            if max_val > 0:\n",
    "                heatmap = heatmap / max_val\n",
    "\n",
    "            heatmap_np = heatmap.numpy()\n",
    "            height, width = heatmap_np.shape\n",
    "\n",
    "            for column, coords in column_coords.items():\n",
    "                x, y = coords\n",
    "\n",
    "                heatmap_x = int(x / pixels * width)\n",
    "                heatmap_y = int(y / pixels * height)\n",
    "\n",
    "                heatmap_x = np.clip(heatmap_x, 0, width - 1)\n",
    "                heatmap_y = np.clip(heatmap_y, 0, height - 1)\n",
    "\n",
    "                importance_value = heatmap_np[heatmap_y, heatmap_x]\n",
    "\n",
    "                metrics_dict[classes[pred_index]][column].append(importance_value)\n",
    "\n",
    "    metrics_dict = {\n",
    "        class_label: {\n",
    "            feature: float(np.mean(values)) if values else 0.0\n",
    "            for feature, values in feature_dict.items()\n",
    "        }\n",
    "        for class_label, feature_dict in metrics_dict.items()\n",
    "    }\n",
    "\n",
    "    return metrics_dict\n",
    "\n",
    "cnn_metrics_path = f'{dataset_path}/cnn_metrics.npy'\n",
    "if not os.path.exists(cnn_metrics_path):\n",
    "  labels = label_encoder.classes_\n",
    "  heatmap_metrics = get_normalized_heatmap_metrics(\n",
    "      X_test_num,\n",
    "      X_test_img,\n",
    "      model,\n",
    "      'conv2d_8',\n",
    "      labels,\n",
    "      modeloIMG.columns_coords,\n",
    "      \"HYBRID\"\n",
    "  )\n",
    "  np.save(cnn_metrics_path, heatmap_metrics, allow_pickle=True)\n",
    "else:\n",
    "  heatmap_metrics = np.load(cnn_metrics_path, allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 644
    },
    "executionInfo": {
     "elapsed": 1016,
     "status": "ok",
     "timestamp": 1757264437551,
     "user": {
      "displayName": "Felipe Escalera Gonzalez",
      "userId": "16752102397912297330"
     },
     "user_tz": -120
    },
    "id": "X8DcTKk-NUaW",
    "outputId": "b89f98e5-9c66-49f7-b855-745502e9ab1c"
   },
   "outputs": [],
   "source": [
    "all_values = [val for class_dict in heatmap_metrics.values() for val in class_dict.values()]\n",
    "min_val, max_val = min(all_values), max(all_values)\n",
    "\n",
    "heatmap_metrics_normalized = {\n",
    "    feature: {c: (val - min_val) / (max_val - min_val) for c, val in class_dict.items()}\n",
    "    for feature, class_dict in heatmap_metrics.items()\n",
    "}\n",
    "\n",
    "plot_stacked_feature_relevance_bar(heatmap_metrics_normalized, title=\"\", output_path=f'{dataset_path}/cnn_metrics.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W_wyosU3NWle"
   },
   "outputs": [],
   "source": [
    "def save_and_display_gradcam(img, heatmap, cam_path=\"cam.jpg\", alpha=0.4):\n",
    "    img = keras.preprocessing.image.img_to_array(img)\n",
    "\n",
    "    heatmap = tf.maximum(heatmap, 0)\n",
    "    max_val = tf.math.reduce_max(heatmap)\n",
    "    if max_val != 0:\n",
    "        heatmap = heatmap / max_val\n",
    "    heatmap = np.uint8(255 * heatmap.numpy())\n",
    "\n",
    "    jet = plt.get_cmap(\"jet\")\n",
    "\n",
    "    jet_colors = jet(np.arange(256))[:, :3]\n",
    "    jet_heatmap = jet_colors[heatmap]\n",
    "\n",
    "    jet_heatmap = keras.preprocessing.image.array_to_img(jet_heatmap)\n",
    "    jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\n",
    "    jet_heatmap = keras.preprocessing.image.img_to_array(jet_heatmap)\n",
    "\n",
    "    superimposed_img = jet_heatmap * alpha + img\n",
    "    superimposed_img = keras.preprocessing.image.array_to_img(superimposed_img)\n",
    "\n",
    "    superimposed_img.save(cam_path)\n",
    "\n",
    "    return superimposed_img\n",
    "\n",
    "def generate_heatmap_and_image(X_val_num, X_val_img, modelX, layer_name, pred_index, model_type=\"HYBRID\"):\n",
    "    if model_type == \"HYBRID\":\n",
    "      heatmap = make_gradcam_heatmap([np.expand_dims(X_val_num, axis=0), np.expand_dims(X_val_img, axis=0)], modelX, layer_name, pred_index=pred_index)\n",
    "    elif model_type == \"CNN\":\n",
    "      img_input = np.expand_dims(X_val_img, axis=0)\n",
    "      img_input = img_input / 255.0\n",
    "      heatmap = make_gradcam_heatmap(img_input[1], modelX, layer_name, pred_index=pred_index)\n",
    "    return save_and_display_gradcam(X_val_img, heatmap)\n",
    "\n",
    "def display_image(ax, row, col, image, title):\n",
    "    axis = ax[row][col] if ax.ndim > 1 else ax[col]\n",
    "    axis.imshow(image)\n",
    "    axis.set_title(title, fontsize=14)\n",
    "    axis.set_xticks([])\n",
    "    axis.set_yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 415
    },
    "executionInfo": {
     "elapsed": 546,
     "status": "ok",
     "timestamp": 1757264438100,
     "user": {
      "displayName": "Felipe Escalera Gonzalez",
      "userId": "16752102397912297330"
     },
     "user_tz": -120
    },
    "id": "bv5KRXFlxCIL",
    "outputId": "0e66f713-f24a-440c-892b-feb62a98207a"
   },
   "outputs": [],
   "source": [
    "samples = 1\n",
    "fig, ax = plt.subplots(1, 2, figsize=(9, 4), squeeze=False)\n",
    "i = 7\n",
    "\n",
    "X_test_data = []\n",
    "label = \"Original Image\"\n",
    "X_test_data.append((X_test_num.iloc[i], X_test_img[i], label))\n",
    "num, img, title = X_test_data[0]\n",
    "display_image(ax, 0, 0, img, title)\n",
    "\n",
    "layer = \"conv2d_1\"\n",
    "superimposed_img = generate_heatmap_and_image(num, img, model, layer, 0, \"HYBRID\")\n",
    "display_title = \"Grad-CAM\"\n",
    "display_image(ax, 0, 1, superimposed_img, display_title)\n",
    "\n",
    "jet_cmap = plt.colormaps['jet']\n",
    "\n",
    "jet_colors = jet_cmap(range(256))\n",
    "jet_colors[:, -1] = 0.4\n",
    "transparent_jet = colors.ListedColormap(jet_colors)\n",
    "\n",
    "norm = colors.Normalize(vmin=0, vmax=1)\n",
    "sm = cm.ScalarMappable(cmap=transparent_jet, norm=norm)\n",
    "sm.set_array([])\n",
    "\n",
    "cbar = fig.colorbar(sm, ax=ax[0, 1], orientation='vertical', fraction=0.046, pad=0.04)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{dataset_path}/feature_importance_sample.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t0aW4GM5Negm"
   },
   "source": [
    "---\n",
    "## <font color=\"#004D7F\" size=6> 6.3. PermGrad Feature Importance</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "haasBex8NfKW"
   },
   "outputs": [],
   "source": [
    "joint_importances_named_path = f'{dataset_path}/joint_metrics.npy'\n",
    "\n",
    "if not os.path.exists(joint_importances_named_path):\n",
    "  joint_metrics = {}\n",
    "\n",
    "  print(\"--- Calculating Baseline Performance ---\")\n",
    "  y_probas_baseline = model.predict([X_test_num, X_test_img], verbose=0)\n",
    "  baseline_loss = np.mean(\n",
    "      tf.keras.losses.categorical_crossentropy(y_test_oh, y_probas_baseline)\n",
    "  )\n",
    "  print(f\"Baseline test loss: {baseline_loss:.4f}\\n\")\n",
    "\n",
    "  numeric_input, image_input = model.inputs\n",
    "\n",
    "  mlp_output = model.get_layer('dropout_13').output\n",
    "  cnn_output = model.get_layer('flatten_2').output\n",
    "\n",
    "  def forward_from(layer_name, new_input):\n",
    "      x = new_input\n",
    "      found = False\n",
    "      for layer in model.layers:\n",
    "          if layer.name == layer_name:\n",
    "              found = True\n",
    "              continue\n",
    "          if found:\n",
    "              x = layer(x)\n",
    "      return x\n",
    "\n",
    "  zeros_for_cnn = Lambda(lambda x: tf.zeros_like(x), name='zeros_for_cnn')(cnn_output)\n",
    "  combined_cnn = model.get_layer('concatenate_2')([mlp_output, zeros_for_cnn])\n",
    "\n",
    "  final_output_cnn = forward_from('concatenate_2', combined_cnn)\n",
    "  ablation_model_cnn = Model(\n",
    "      inputs=[numeric_input, image_input], outputs=final_output_cnn\n",
    "  )\n",
    "\n",
    "  y_probas_cnn = ablation_model_cnn.predict([X_test_num, X_test_img], verbose=0)\n",
    "  loss_cnn = np.mean(tf.keras.losses.categorical_crossentropy(y_test_oh, y_probas_cnn).numpy())\n",
    "  joint_metrics[\"CNN\"] = float(loss_cnn - baseline_loss)\n",
    "\n",
    "  print(\"CNN branch ablation:\")\n",
    "  print(f\"  Ablated loss: {loss_cnn:.4f}\")\n",
    "  print(f\"  ΔLoss (Change in Loss): {joint_metrics['CNN']:.4f}\\n\")\n",
    "\n",
    "  zeros_for_mlp = Lambda(lambda x: tf.zeros_like(x), name='zeros_for_mlp')(mlp_output)\n",
    "  combined_mlp = model.get_layer('concatenate_2')([zeros_for_mlp, cnn_output])\n",
    "\n",
    "  final_output_mlp = forward_from('concatenate_2', combined_mlp)\n",
    "  ablation_model_mlp = Model(\n",
    "      inputs=[numeric_input, image_input], outputs=final_output_mlp\n",
    "  )\n",
    "\n",
    "  y_probas_mlp = ablation_model_mlp.predict([X_test_num, X_test_img], verbose=0)\n",
    "  loss_mlp = np.mean(tf.keras.losses.categorical_crossentropy(y_test_oh, y_probas_mlp).numpy())\n",
    "  joint_metrics[\"MLP\"] = float(loss_mlp - baseline_loss)\n",
    "\n",
    "  print(\"MLP branch ablation:\")\n",
    "  print(f\"  Ablated loss: {loss_mlp:.4f}\")\n",
    "  print(f\"  ΔLoss (Change in Loss): {joint_metrics['MLP']:.4f}\\n\")\n",
    "\n",
    "  print(\"ΔLoss Results Dictionary:\")\n",
    "  print(joint_metrics)\n",
    "\n",
    "  all_values = list(joint_metrics.values())\n",
    "  den = sum(math.exp(v) for v in all_values)\n",
    "  joint_metrics = {b: math.exp(v) / den for b, v in joint_metrics.items()}\n",
    "\n",
    "  np.save(joint_importances_named_path, joint_metrics, allow_pickle=True)\n",
    "else:\n",
    "  joint_metrics = np.load(joint_importances_named_path, allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Branch Importance:\")\n",
    "print(joint_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u8GBW6YgNi9W"
   },
   "outputs": [],
   "source": [
    "perm_grad_mlp_importances_normalized = {\n",
    "    class_label: {feature: joint_metrics[\"MLP\"] * val\n",
    "                  for feature, val in class_dict.items()}\n",
    "    for class_label, class_dict in mlp_importances_normalized.items()\n",
    "}\n",
    "\n",
    "perm_grad_heatmap_metrics_normalized = {\n",
    "    class_label: {feature: joint_metrics[\"CNN\"] * val\n",
    "                  for feature, val in class_dict.items()}\n",
    "    for class_label, class_dict in heatmap_metrics_normalized.items()\n",
    "}\n",
    "\n",
    "global_importance_by_branch = {}\n",
    "\n",
    "for class_label in perm_grad_mlp_importances_normalized.keys():\n",
    "    global_importance_by_branch[class_label] = defaultdict(float)\n",
    "\n",
    "    for feature, val in perm_grad_mlp_importances_normalized.get(class_label, {}).items():\n",
    "        global_importance_by_branch[class_label][feature] += val\n",
    "\n",
    "    for feature, val in perm_grad_heatmap_metrics_normalized.get(class_label, {}).items():\n",
    "        global_importance_by_branch[class_label][feature] += val\n",
    "\n",
    "    global_importance_by_branch[class_label] = dict(global_importance_by_branch[class_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 644
    },
    "executionInfo": {
     "elapsed": 1001,
     "status": "ok",
     "timestamp": 1757264439138,
     "user": {
      "displayName": "Felipe Escalera Gonzalez",
      "userId": "16752102397912297330"
     },
     "user_tz": -120
    },
    "id": "mgwAP73KNlHW",
    "outputId": "510badb5-5933-4308-9121-5361f465e785"
   },
   "outputs": [],
   "source": [
    "plot_stacked_feature_relevance_bar(global_importance_by_branch, title=\"\", output_path=f'{dataset_path}/permgrad.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1757264439151,
     "user": {
      "displayName": "Felipe Escalera Gonzalez",
      "userId": "16752102397912297330"
     },
     "user_tz": -120
    },
    "id": "W45hkcozFx7a",
    "outputId": "4bf6a00e-6baa-4740-a6fc-72db1f6b5cb4"
   },
   "outputs": [],
   "source": [
    "global_importance_by_branch\n",
    "\n",
    "result = {}\n",
    "for category in global_importance_by_branch.values():\n",
    "    for key, value in category.items():\n",
    "        result[key] = result.get(key, 0) + value\n",
    "\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
